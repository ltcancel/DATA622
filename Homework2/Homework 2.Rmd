---
title: "Homework 2"
author: "LeTicia Cancel"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Based on the latest topics presented, bring a dataset of your choice and create a Decision Tree where you can solve a classification or regression problem and predict the outcome of a particular feature or detail of the data used.
Switch variables to generate 2 decision trees and compare the results. Create a random forest for regression and analyze the results.

Based on real cases where decision trees went wrong, and 'the bad & ugly' aspects of decision trees (https://decizone.com/blog/the-good-the-bad-the-ugly-of-using-decision-trees), how can you change this perception when using the decision tree you created to solve a real problem?

Format: document with screen captures & analysis.



## Libraries
```{r warning=FALSE, message=FALSE}
library(rcompanion)
library(datasets)
library(caTools)
library(dplyr)
library(party)
library(rpart)
library(rpart.plot)
library(caret)
library(ggcorrplot)
```


## Import Data
```{r}
df <- read.csv("https://raw.githubusercontent.com/ltcancel/DATA622/main/Homework2/winequality-red.csv", sep = ";", quote = "")

head(df)
tail(df)
?read.csv
```

The variables in this dataset are:

   1 - fixed acidity
   2 - volatile acidity
   3 - citric acid
   4 - residual sugar
   5 - chlorides
   6 - free sulfur dioxide
   7 - total sulfur dioxide
   8 - density
   9 - pH
   10 - sulphates
   11 - alcohol
   Output variable (based on sensory data): 
   12 - quality (score between 0 and 10)
   
Renaming the variables to make them easier to read
```{r}
colnames(df) <- c("Fixed Acidity","Volatile Acidity","Citric Acid","Residual Sugar","Chlorides","Free Sulfur Dioxide","Total Sulfur Dioxide","Density","pH","Sulphares","Alcohol","Quality")
```

## Data Structure

There are 12 variables. 11 of the 12 are numbers and one variable is an int so we do not have to convert any of the variables to factors for this analysis 

```{r}
str(df)
```

There are no NA values in any of the variables. 

```{r}
summary(df)
```

## correlation Table

Are any of the variables correlated?

Residual Sugar and Volatile Acidity have a correlation coefficient of zero so there is zero correlation between these two variables. 

The top negatively correlated variables are:
- pH and Fixed Acidity -0.68 
- Volatile Acidity and Citric Acid -0.55  
- pH and Citric Acid -0.54
- Alcohol and Density -0.5

The top positively correlated variables are:
- Three variable pairs are tied for the top positive correclation coefficient of 0.67
    - Total Sulfur Dioxide and Free Sulfur Dioxide
    - Citric Acid and Fixed Acidity  
    - Density and Fixed Acidity
- Quality and Alcohol 0.48



```{r fig.width= 13, fig.height=13}
cor_df <- cor(df)

ggcorrplot(cor_df, hc.order = TRUE, type = "upper", lab = TRUE)
#ggcorrplot(cor_df, hc.order = TRUE, type = "upper", p.mat = p.mat)
```


Create the train/test sets
```{r}
create_train_test <- function(data, size = 0.8, train = TRUE){
  n_row = nrow(data)
  total_row = size * n_row
  train_sample <- 1: total_row
  if (train == TRUE){
    return(data[train_sample, ])
  } else {
    return(data[-train_sample, ])
  }
}
```


Test the function and check the dimension
```{r}
data_train <- create_train_test(df, 0.8, train = TRUE)
data_test <- create_train_test(df, 0.8, train = FALSE)
dim(data_train)
dim(data_test)
```

Using the prop.table() function to verify if the randomization process is correct. We are comparing the Quality score values between the train and test data. The values are similar without too much variance so we can continue. 
```{r}
prop.table(table(data_test$Quality))
prop.table(table(data_train$Quality))
```






## Data Comparison

Both dataframes look nearly identical, other than the obvious difference in the number of observations. If we look a little closer to the summary of each data set we can find some more differences. The minimum values for the Total.Revenue variable differs. The minimum for the first dataframe is 2043 and the minimum for the second dataframe is 19. Total.Cost and Total.Profit also show a significant difference with a minimum value of 1417 and 532.6 for respectively for the first dataframe and 14 and 4.8 for the second dataframe. Neither dataframe has NA values. The minimum and maximum values do not look off. They all look like valid values based on the variable names. 

From the summaries below we can see that both datasets seem relatively simple with mostly categorical data so I will use Decision Tree and SVM models. 

```{r}
#check for NA values
summary(df100)
```

```{r}
summary(df1000)
```

## Decision Tree

The modeling for this data is very experimental because it is not clear what the business problem or question is. What are we attempting to solve for? Since we have two clear distinct categorical variables, Sales Channel and Order Priority, we can use one to be our predictor variable. 

We will use a Decision Tree as our modeling technique for both datasets. 

```{r}
df100$Sales.Channel <- as.factor(df100$Sales.Channel)
df100$Order.Priority <- as.factor(df100$Order.Priority)
df1000$Region <- as.factor(df1000$Region)
df1000$Country <- as.factor(df1000$Country)
df1000$Item.Type <- as.factor(df1000$Item.Type)
df1000$Sales.Channel <- as.factor(df1000$Sales.Channel)
df1000$Order.Priority <- as.factor(df1000$Order.Priority)
str(df1000)
```

Correlation table
```{r}
head(df1000)

head(cor1)
str(cor1)
cor.test(cor1$Sales.Channel, cor1$Units.Sold)
table(cor1)
```

Chi-Square Test of all factor variables
```{r}
chisq.test(df1000$Region,df1000$Country)
chisq.test(df1000$Region,df1000$Item.Type)
chisq.test(df1000$Region,df1000$Sales.Channel)
chisq.test(df1000$Region,df1000$Order.Priority)
```

Chi-Square Test using Region
```{r}
chisq.test(df1000$Region,df1000$Country)
chisq.test(df1000$Region,df1000$Item.Type)
chisq.test(df1000$Region,df1000$Sales.Channel)
chisq.test(df1000$Region,df1000$Order.Priority)
chisq.test(df1000$Region,df1000$Units.Sold)
chisq.test(df1000$Region,df1000$Unit.Price)
chisq.test(df1000$Region,df1000$Unit.Cost)
chisq.test(df1000$Region,df1000$Total.Revenue)
chisq.test(df1000$Region,df1000$Total.Cost)
chisq.test(df1000$Region,df1000$Total.Profit)
```

Chi-Square Test using Country
```{r}
chisq.test(df1000$Country,df1000$Region)
chisq.test(df1000$Country,df1000$Item.Type)
chisq.test(df1000$Country,df1000$Sales.Channel)
chisq.test(df1000$Country,df1000$Order.Priority)
chisq.test(df1000$Country,df1000$Units.Sold)
chisq.test(df1000$Country,df1000$Unit.Price)
chisq.test(df1000$Country,df1000$Unit.Cost)
chisq.test(df1000$Country,df1000$Total.Revenue)
chisq.test(df1000$Country,df1000$Total.Cost)
chisq.test(df1000$Country,df1000$Total.Profit)
```

Chi-Square Test using Sales Channel

```{r}
chisq.test(df1000$Sales.Channel,df1000$Country)
chisq.test(df1000$Sales.Channel,df1000$Item.Type)
chisq.test(df1000$Sales.Channel,df1000$Order.Priority)
chisq.test(df1000$Sales.Channel,df1000$Units.Sold)
chisq.test(df1000$Sales.Channel,df1000$Unit.Price)
chisq.test(df1000$Sales.Channel,df1000$Unit.Cost)
chisq.test(df1000$Sales.Channel,df1000$Total.Revenue)
chisq.test(df1000$Sales.Channel,df1000$Total.Cost)
chisq.test(df1000$Sales.Channel,df1000$Total.Profit)
```

Chi-Square Test using Order Priority
```{r}
chisq.test(df1000$Order.Priority,df1000$Sales.Channel)
chisq.test(df1000$Order.Priority,df1000$Country)
chisq.test(df1000$Order.Priority,df1000$Item.Type)
chisq.test(df1000$Order.Priority,df1000$Order.Priority)
chisq.test(df1000$Order.Priority,df1000$Units.Sold)
chisq.test(df1000$Order.Priority,df1000$Unit.Price)
chisq.test(df1000$Order.Priority,df1000$Unit.Cost)
chisq.test(df1000$Order.Priority,df1000$Total.Revenue)
chisq.test(df1000$Order.Priority,df1000$Total.Cost)
chisq.test(df1000$Order.Priority,df1000$Total.Profit)
```

```{r}
cor1 <- df1000 %>%
  select(-c(Region,Country,Item.Type,Sales.Channel,Order.Priority,Ship.Date,Order.Date,Order.ID))
cor(df1000$Units.Sold, df1000$Unit.Price)
cor.test(df1000$Units.Sold, df1000$Unit.Cost)
cor.test(df1000$Unit.Price, df1000$Unit.Price)
as.dist(round(cor(cor1, method = "spearman"),4))
```


```{r}
#sample set
df100_data <- df100 %>%
  select(Order.Priority, Units.Sold, Unit.Price, Unit.Cost, Total.Revenue, Total.Cost, Total.Profit)

set.seed(1234)

#dt <- sort(sample(nrow(df100_data),nrow(df100_data)*.7))
#train_data <- df100_data[df,]

sample_data <- sample.split(df100_data, SplitRatio = 0.80)
train_data <- subset(df100_data, sample_data == TRUE)
test_data <- subset(df100_data, sample_data == FALSE)
head(train_data)
```

Build tree for first set
```{r}
rtree <- rpart(Order.Priority ~ ., data = train_data, method="class", control = rpart.control(minsplit = 20, minbucket = 7, maxdepth = 10, usesurrogate = 2, xval = 10))
rtree
```

Plot the tree
```{r}
rpart.plot(rtree)
```

Prediction of first dataset
```{r}
pred <- predict(rtree, test_data, type = "class")
pred_table <- table(test_data$Order.Priority, pred)
pred_table
```

Confusion Matrix to test for accuracy
```{r}
confusionMatrix(test_data$Order.Priority, pred)
```

We will build the same decision tree with the second dataset

```{r}
#sample set
df1000_data <- df1000 %>%
  select(Order.Priority, Units.Sold, Unit.Price, Unit.Cost, Total.Revenue, Total.Cost, Total.Profit)

set.seed(1234)


sample_data2 <- sample.split(df1000_data, SplitRatio = 0.80)
train_data2 <- subset(df1000_data, sample_data2 == TRUE)
test_data2 <- subset(df1000_data, sample_data2 == FALSE)
head(train_data2)
```

Build tree for second set
```{r}
rtree2 <- rpart(Order.Priority ~ ., data = train_data2, method="class", control = rpart.control(minsplit = 4, minbucket = round(5/3), maxdepth = 3))
rtree2
```

Plot the tree for second set
```{r}
#rpart.plot(rtree2)
```



